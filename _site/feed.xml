<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-06-08T23:28:35-06:00</updated><id>http://localhost:4000/</id><title type="html">Machine Learning | Topological Defects</title><subtitle>We are a group of graduate and undergradate students at University of Colorado Boulder, investigating the application of machine learning to the physics of 2D liquid crystal films</subtitle><author><name>Adam Green</name></author><entry><title type="html">Using Deep Learning Algorithms for Defect Detection</title><link href="http://localhost:4000/2019/06/03/mldescription.html" rel="alternate" type="text/html" title="Using Deep Learning Algorithms for Defect Detection" /><published>2019-06-03T00:00:00-06:00</published><updated>2019-06-03T00:00:00-06:00</updated><id>http://localhost:4000/2019/06/03/mldescription</id><content type="html" xml:base="http://localhost:4000/2019/06/03/mldescription.html">&lt;h2 id=&quot;algorithms&quot;&gt;Algorithms&lt;/h2&gt;
&lt;p&gt;A great deal of research has been performed on applying deep learning to object detection tasks. The resulting methods have proved to be quicker and more accurate than traditional computer vision techniques, however adoption of these algorithms for laboratory usage has lagged due to the difficulty in acquiring the large, labeled datasets necessary for training neural networks. We will be attempting to determine the efficacy of generating labeled training data from a simulation, and using it to detect defects in real-world data. Ideally, we would also like to create a simple pipeline allowing the detection of other objects to be quickly automated.&lt;/p&gt;

&lt;p&gt;For reference, the images we are attempting to annotate look like the one below. Defects are the points marked by red dots.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/153248_defect11SIMMARKED.jpg&quot; alt=&quot;Sample Defect Image &quot; title=&quot;Sample Defect Image&quot; class=&quot;center-image&quot; /&gt;
&lt;em&gt;Sample Defect Image&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The three most popular algorithms for object detection are RCNN (and its successors), Single Shot Detectors (SSD), and YOLO. Although all three approaches make use of deep neural networks, their methods of finding bounding boxes differ greatly. It is important to understand these differences, so I will provide a brief summary of each.
R-CNN – Region Convolutional Neural network. Most modern version is known as Faster R-CNN. This algorithm uses a search function to find potential candidate objects to run object recognition on. Slower, but accurate. Unfortunately, since this algorithm depends on only running the neural network on likely object candidates detected in a untrainable search algorithm, it will most likely perform poorly at detecting objects that don’t fit traditional ideas of what constitutes an object.&lt;/p&gt;

&lt;p&gt;SSD- Single Shot (multibox) Detector. This algorithm works by creating multiple default bounding boxes over the entirety of an image. A neural network scores the bounding boxes and provides a prediction for how to adjust the regions to maximize a high class. Faster than R-CNN, but slightly less accurate.&lt;/p&gt;

&lt;p&gt;YOLO – You Only Look Once. The input image is initially divided into a grid. Each cell in the grid is used to predict five bounding boxes. A classifier is run for each predicted bounding box, and outcomes with high certainty scores are returned as objects. This is the fastest of the three algorithms, although slightly less accurate in general. However, since the features we are looking at aren’t typical objects this method is likely to work better than R-CNN.&lt;/p&gt;

&lt;p&gt;Since we will be attempting to detect defects, which do not look like the ordinary definition of an object (see above image), R-CNN would make a poor choice. Both SSD and YOLO are likely to produce good results. YOLO has the advantage of the having a highly accessibly implementation known as darkflow, which uses the python tensorflow library on the backend. This a major advantage when considering accessibility for scientists or industrial users who may need to make their own modifications, as python is a much more readable and easier to understand language than other languages commonly used for deep learning algorithms.&lt;/p&gt;

&lt;h2 id=&quot;darkflow&quot;&gt;Darkflow&lt;/h2&gt;
&lt;p&gt;The original darkflow repository can be found here https://github.com/thtrieu/darkflow
The repository for our modified code can be found here https://github.com/mlfilms/defectTracker
Our code includes scripts for simplifying the process of training and running the algorithm.
To make a model, you will also need the default weights file to train from, which is located at https://drive.google.com/drive/u/2/folders/1c_xrWVKNBuqZUwXGvv_MBamOHNa1wBKy along with other trained models. Download the yolo.weights file and place it in a bin folder.&lt;/p&gt;

&lt;p&gt;The readme included in the repo explains how to use the system in detail, and includes instructions on installing dependencies.&lt;/p&gt;

&lt;p&gt;Inside the darkflow folder are three scripts that make running training, running, and validating the model easier. trainFlow.py uses annotations and images to train a model. Adjust the batch size or gpu usage number if you run into memory errors. runFlowPB.py loads a model from the .pb and .meta files generated when a model is finished training. The images in the specified directory will be analyzed and marked images will be output, along with json files containing the detections. Finally, validation .py makes use of code from https://github.com/Cartucho/mAP to evaluate how well a trained model performs&lt;/p&gt;

&lt;p&gt;Also included are scripts for generating simple training data datasets consisting of images with circles. As a first order test, this simulated data was used to evaluate the effectiveness of YOLO for detecting circles when compared with a common, non-ML algorithm, the circular hough transform. Although both methods were able to properly label almost all circles in clear images, YOLO showed superiority at detecting circles in noisy data, yielding a mAP score of around 80% when the hough transform failed to detect any circles.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/circle_14.jpg&quot; alt=&quot;Noisy Circles &quot; title=&quot;Noisy Circles&quot; class=&quot;center-image&quot; width=&quot;400px&quot; /&gt;
&lt;em&gt;The YOLO detections in noisy data. The circular hough transform failed to detect any circles in this data.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/circle_4.jpg&quot; alt=&quot;Clear Circles &quot; title=&quot;Clear Circles&quot; class=&quot;center-image&quot; width=&quot;400px&quot; /&gt;
&lt;em&gt;Clear images of circles. Both the circular hough transform and YOLO were able to detect almost all images in this dataset.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This demonstrates the viability of YOLO at providing superior performance detecting objects in imperfect or noisy data, as is often necessary in real scenarios.&lt;/p&gt;

&lt;p&gt;Training the model took around 45 minutes on a GTX 1080 GPU, and the final model was able to label 200 images in about 25 seconds, demonstrating the viability of training and using darkflow without the need for specialized hardward.&lt;/p&gt;

&lt;p&gt;The system can be setup on a windows computer running anaconda in three commands, and can be retrained, ran, and validated with easily with three scripts written to simplify the process.&lt;/p&gt;

&lt;p&gt;This allowed a new model to be quickly trained and run on simulated data for defects. The defect simulation and physical significance will be explained in detail in a later post. The initial results are promising on both noisy and clean data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2485_defect63.jpg&quot; alt=&quot;Noisy Defects &quot; title=&quot;Noisy Defects&quot; class=&quot;center-image&quot; width=&quot;400px&quot; /&gt;
&lt;em&gt;Noisy, crowded simulated defect data. The red markers are all the defect detections from YOLO. Although almost no defects are missed, several false detections exist.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/153248_defect8.jpg&quot; alt=&quot;Clear Defects &quot; title=&quot;Clear Defects&quot; class=&quot;center-image image-xl&quot; width=&quot;400px&quot; /&gt;.
&lt;em&gt;Clean simulated defect data. The trained model was able to accurately label all defects with few mistakes.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The next step is to asssess the viability of the models trained on simulated data at detecting defects in experimental data.&lt;/p&gt;</content><author><name>Eric</name></author><summary type="html">Algorithms A great deal of research has been performed on applying deep learning to object detection tasks. The resulting methods have proved to be quicker and more accurate than traditional computer vision techniques, however adoption of these algorithms for laboratory usage has lagged due to the difficulty in acquiring the large, labeled datasets necessary for training neural networks. We will be attempting to determine the efficacy of generating labeled training data from a simulation, and using it to detect defects in real-world data. Ideally, we would also like to create a simple pipeline allowing the detection of other objects to be quickly automated. For reference, the images we are attempting to annotate look like the one below. Defects are the points marked by red dots. Sample Defect Image The three most popular algorithms for object detection are RCNN (and its successors), Single Shot Detectors (SSD), and YOLO. Although all three approaches make use of deep neural networks, their methods of finding bounding boxes differ greatly. It is important to understand these differences, so I will provide a brief summary of each. R-CNN – Region Convolutional Neural network. Most modern version is known as Faster R-CNN. This algorithm uses a search function to find potential candidate objects to run object recognition on. Slower, but accurate. Unfortunately, since this algorithm depends on only running the neural network on likely object candidates detected in a untrainable search algorithm, it will most likely perform poorly at detecting objects that don’t fit traditional ideas of what constitutes an object. SSD- Single Shot (multibox) Detector. This algorithm works by creating multiple default bounding boxes over the entirety of an image. A neural network scores the bounding boxes and provides a prediction for how to adjust the regions to maximize a high class. Faster than R-CNN, but slightly less accurate. YOLO – You Only Look Once. The input image is initially divided into a grid. Each cell in the grid is used to predict five bounding boxes. A classifier is run for each predicted bounding box, and outcomes with high certainty scores are returned as objects. This is the fastest of the three algorithms, although slightly less accurate in general. However, since the features we are looking at aren’t typical objects this method is likely to work better than R-CNN. Since we will be attempting to detect defects, which do not look like the ordinary definition of an object (see above image), R-CNN would make a poor choice. Both SSD and YOLO are likely to produce good results. YOLO has the advantage of the having a highly accessibly implementation known as darkflow, which uses the python tensorflow library on the backend. This a major advantage when considering accessibility for scientists or industrial users who may need to make their own modifications, as python is a much more readable and easier to understand language than other languages commonly used for deep learning algorithms. Darkflow The original darkflow repository can be found here https://github.com/thtrieu/darkflow The repository for our modified code can be found here https://github.com/mlfilms/defectTracker Our code includes scripts for simplifying the process of training and running the algorithm. To make a model, you will also need the default weights file to train from, which is located at https://drive.google.com/drive/u/2/folders/1c_xrWVKNBuqZUwXGvv_MBamOHNa1wBKy along with other trained models. Download the yolo.weights file and place it in a bin folder. The readme included in the repo explains how to use the system in detail, and includes instructions on installing dependencies. Inside the darkflow folder are three scripts that make running training, running, and validating the model easier. trainFlow.py uses annotations and images to train a model. Adjust the batch size or gpu usage number if you run into memory errors. runFlowPB.py loads a model from the .pb and .meta files generated when a model is finished training. The images in the specified directory will be analyzed and marked images will be output, along with json files containing the detections. Finally, validation .py makes use of code from https://github.com/Cartucho/mAP to evaluate how well a trained model performs Also included are scripts for generating simple training data datasets consisting of images with circles. As a first order test, this simulated data was used to evaluate the effectiveness of YOLO for detecting circles when compared with a common, non-ML algorithm, the circular hough transform. Although both methods were able to properly label almost all circles in clear images, YOLO showed superiority at detecting circles in noisy data, yielding a mAP score of around 80% when the hough transform failed to detect any circles. The YOLO detections in noisy data. The circular hough transform failed to detect any circles in this data. Clear images of circles. Both the circular hough transform and YOLO were able to detect almost all images in this dataset. This demonstrates the viability of YOLO at providing superior performance detecting objects in imperfect or noisy data, as is often necessary in real scenarios. Training the model took around 45 minutes on a GTX 1080 GPU, and the final model was able to label 200 images in about 25 seconds, demonstrating the viability of training and using darkflow without the need for specialized hardward. The system can be setup on a windows computer running anaconda in three commands, and can be retrained, ran, and validated with easily with three scripts written to simplify the process. This allowed a new model to be quickly trained and run on simulated data for defects. The defect simulation and physical significance will be explained in detail in a later post. The initial results are promising on both noisy and clean data. Noisy, crowded simulated defect data. The red markers are all the defect detections from YOLO. Although almost no defects are missed, several false detections exist. . Clean simulated defect data. The trained model was able to accurately label all defects with few mistakes. The next step is to asssess the viability of the models trained on simulated data at detecting defects in experimental data.</summary></entry><entry><title type="html">Welcome</title><link href="http://localhost:4000/2019/05/31/welcome.html" rel="alternate" type="text/html" title="Welcome" /><published>2019-05-31T00:00:00-06:00</published><updated>2019-05-31T00:00:00-06:00</updated><id>http://localhost:4000/2019/05/31/welcome</id><content type="html" xml:base="http://localhost:4000/2019/05/31/welcome.html">&lt;p&gt;We are a group of graduate and undergraduate students at the University of Colorado Boulder, investigating the use of machine learning in experimental physics.&lt;/p&gt;

&lt;p&gt;Follow our journey in real time!&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;what?&lt;/h2&gt;

&lt;p&gt;As part of his thesis, Adam Green was studying the interaction of topological defects in 2D systems. In the lab, we can create quasi-2D fluids by drawing a liquid crystal film over an aperature. We then inject energy into the system (read: hit it really hard), which creates little ‘kinks’ in the fluid, which we can study using polarized-light microscopy. To put it shortly, the interaction of these kinks is interesting to some people (read: physicists).&lt;/p&gt;

&lt;h2 id=&quot;why-machine-learning-though&quot;&gt;why machine learning though&lt;/h2&gt;
&lt;p&gt;This is what our experimental data looks like.&lt;/p&gt;

&lt;p&gt;Previous efforts have required the hand-tagging of each defect. Once that is done, statistics can be run on the collection. However, because this is such an arduous process, it severly limits how much data can be analyzed.&lt;/p&gt;

&lt;p&gt;Also, I didn’t want to spend months squinting over a screen, clicking at pixels.&lt;/p&gt;

&lt;p&gt;So, this seemed like a natural job for machine learning.&lt;/p&gt;

&lt;h2 id=&quot;but-wait&quot;&gt;but wait…&lt;/h2&gt;
&lt;p&gt;If you are familiar with machine learning, you know that you have to feed your little robot with training data– ie. data that is tagged.&lt;/p&gt;

&lt;p&gt;So have we really won here? We still have to go into experimental data and hand tag it to generate training data.&lt;/p&gt;

&lt;p&gt;Well…yes and no. Of course, at the end of the day, we do need some hand-tagged experimental images to verify that our process is giving good results, but because the physical system is fairly simple, we may be able to simulate ‘fake’ experiments on the computer and generate training data.&lt;/p&gt;

&lt;p&gt;2D tilted liquid crystal films can be described by the 2D XY model&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, which has a rich history in physics. We will discuss the 2D XY model in a future post, but for now just know that there are methods to numerically solve the 2D XY model. We can use these methods to generate ‘fake’ experiments, which we can then use as training data. At least, that’s the hope.&lt;/p&gt;

&lt;!--- If you like TeXt, don't forget to give me a star. :star2:

 [![Star This Project](https://img.shields.io/github/stars/kitian616/jekyll-TeXt-theme.svg?label=Stars&amp;style=social)](https://github.com/kitian616/jekyll-TeXt-theme/)
 ---&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Not that well though, as the XY model completely neglects hydrodynamics and the liquid-crystal film is, well, liquidy. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>adam</name></author><category term="xymodel," /><category term="ml" /><summary type="html">We are a group of graduate and undergraduate students at the University of Colorado Boulder, investigating the use of machine learning in experimental physics. Follow our journey in real time! what? As part of his thesis, Adam Green was studying the interaction of topological defects in 2D systems. In the lab, we can create quasi-2D fluids by drawing a liquid crystal film over an aperature. We then inject energy into the system (read: hit it really hard), which creates little ‘kinks’ in the fluid, which we can study using polarized-light microscopy. To put it shortly, the interaction of these kinks is interesting to some people (read: physicists). why machine learning though This is what our experimental data looks like. Previous efforts have required the hand-tagging of each defect. Once that is done, statistics can be run on the collection. However, because this is such an arduous process, it severly limits how much data can be analyzed. Also, I didn’t want to spend months squinting over a screen, clicking at pixels. So, this seemed like a natural job for machine learning. but wait… If you are familiar with machine learning, you know that you have to feed your little robot with training data– ie. data that is tagged. So have we really won here? We still have to go into experimental data and hand tag it to generate training data. Well…yes and no. Of course, at the end of the day, we do need some hand-tagged experimental images to verify that our process is giving good results, but because the physical system is fairly simple, we may be able to simulate ‘fake’ experiments on the computer and generate training data. 2D tilted liquid crystal films can be described by the 2D XY model1, which has a rich history in physics. We will discuss the 2D XY model in a future post, but for now just know that there are methods to numerically solve the 2D XY model. We can use these methods to generate ‘fake’ experiments, which we can then use as training data. At least, that’s the hope. Not that well though, as the XY model completely neglects hydrodynamics and the liquid-crystal film is, well, liquidy. &amp;#8617;</summary></entry><entry><title type="html">Post with Header Image</title><link href="http://localhost:4000/2018/06/01/header-image.html" rel="alternate" type="text/html" title="Post with Header Image" /><published>2018-06-01T00:00:00-06:00</published><updated>2018-06-01T00:00:00-06:00</updated><id>http://localhost:4000/2018/06/01/header-image</id><content type="html" xml:base="http://localhost:4000/2018/06/01/header-image.html">&lt;p&gt;A Post with Header Image, See &lt;a href=&quot;https://tianqi.name/jekyll-TeXt-theme/samples.html#page-layout&quot;&gt;Page layout&lt;/a&gt; for more examples.&lt;/p&gt;

&lt;!--more--&gt;</content><author><name>Adam Green</name></author><category term="TeXt" /><summary type="html">A Post with Header Image, See Page layout for more examples.</summary></entry></feed>